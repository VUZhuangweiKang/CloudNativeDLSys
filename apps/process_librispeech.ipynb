{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess LibriSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torchaudio\n",
    "import json\n",
    "\n",
    "\n",
    "wav_files = glob.glob('./wav/*.wav')\n",
    "with open('../labels.json') as f:\n",
    "    labels = json.load(f)    \n",
    "labels_map = dict([(labels[i], i) for i in range(len(labels))])\n",
    "\n",
    "def load_audio(path):\n",
    "    sound, sample_rate = torchaudio.load(path)\n",
    "    if sound.shape[0] == 1:\n",
    "        sound = sound.squeeze()\n",
    "    else:\n",
    "        sound = sound.mean(axis=0)  # multiple channels, average\n",
    "    return sound.numpy()\n",
    "\n",
    "def parse_transcript(transcript_path):\n",
    "    with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "        transcript = transcript_file.read().replace('\\n', '')\n",
    "    transcript = list(filter(None, [labels_map.get(x) for x in list(transcript)]))\n",
    "    return transcript\n",
    "\n",
    "# combine wav and txt\n",
    "for wav_file in wav_files:\n",
    "    txt_file = wav_file.replace('wav', 'txt')\n",
    "    tag = txt_file.split('/')[-1].split('.')[0]\n",
    "    wav = load_audio(wav_file)\n",
    "    txt = parse_transcript(txt_file)\n",
    "    np.save(f\"./wavtxt/{tag}.npy\", np.array([wav, txt]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "import concurrent\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "CHUNK_SIZE = int(128*1e6)\n",
    "root = '.'\n",
    "croot = './tar'\n",
    "ds = 'train'\n",
    "\n",
    "files = glob.glob('%s/%s/*.npy' % (root, ds))\n",
    "files.sort()\n",
    "\n",
    "def compressor(chunk, files_chunk):\n",
    "    dst = \"{}/{}\".format(croot, ds)\n",
    "    try:\n",
    "        os.makedirs(dst)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    s = 0\n",
    "    mytar = tarfile.open(\"{}/{}.tar.gz\".format(dst, chunk),\"w:gz\")\n",
    "    for f in files_chunk:\n",
    "        s += os.path.getsize(f)\n",
    "        if s < CHUNK_SIZE:\n",
    "            mytar.add(f, arcname=f.split(\"/\")[-1])\n",
    "        else:\n",
    "            s = 0\n",
    "            mytar.close()\n",
    "            mytar = tarfile.open(\"{}/{}.tar.gz\".format(dst, chunk),\"w:gz\")\n",
    "    mytar.close()\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def compress_data():\n",
    "    with Pool(cpu_count()) as p:\n",
    "        num_files_per_process = len(files) // cpu_count()  # divide files equally among processes\n",
    "        chunked_files = list(chunks(files, num_files_per_process))\n",
    "        p.starmap(compressor, enumerate(chunked_files))\n",
    "\n",
    "compress_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import glob\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "\n",
    "session = boto3.Session()\n",
    "s3 = session.client(\"s3\")\n",
    "bucket = 'vuzhuangwei'\n",
    "\n",
    "def preprocess(path):\n",
    "    key = 'LibriSpeech-Mini/{}'.format(path)\n",
    "    s3.upload_file(path, bucket, key)\n",
    "    print(key)\n",
    " \n",
    "def upload_objects(folder):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
    "        futures = []\n",
    "        if os.path.isdir(folder):\n",
    "            imgs = glob.glob('{}/*'.format(folder))\n",
    "        else:\n",
    "            imgs = [folder]\n",
    "        for path in imgs:\n",
    "            futures.append(executor.submit(preprocess, path))\n",
    "        concurrent.futures.wait(futures)\n",
    "        \n",
    "\n",
    "upload_objects(\"train/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
